{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install picsellia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from picsellia import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = \"your_token\" # API Token from the picsell-IA platform\n",
    "project_token = \"your_project_token\" # project token dounf in project -> settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"your_model_name\" # Name your soon-to-be trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clt = Client(api_token=api_token)\n",
    "clt.init_project(project_token=project_token)\n",
    "clt.init_model(model_name)\n",
    "clt.dl_annotations()\n",
    "clt.generate_labelmap()\n",
    "clt.local_pic_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_record_files(label_map, record_dir, tfExample_generator, annotation_type):\n",
    "    ensembles = [\"train\", \"eval\"]    \n",
    "    for ensemble in ensembles:\n",
    "        output_path = record_dir+ensemble+\".record\"\n",
    "        writer = tf.io.TFRecordWriter(output_path)\n",
    "        for variables in tfExample_generator(label_map, ensemble=ensemble, annotation_type=annotation_type):\n",
    "            (width, height, filename, encoded_jpg, image_format, \n",
    "                classes_text, classes) = variables\n",
    "\n",
    "            tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'image/encoded': _bytes_feature(encoded_jpg),\n",
    "                'image/object/class/label': _int64_feature(classes[0]-1)\n",
    "                }))\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "    \n",
    "        writer.close()\n",
    "        print('Successfully created the TFRecords: {}'.format(output_path))\n",
    "\n",
    "annotation_type = \"classification\"                \n",
    "create_record_files(label_map=clt.label_map, record_dir=clt.record_dir, \n",
    "                    tfExample_generator=clt.tf_vars_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "      'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "      'image/object/class/label': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.Example` proto using the dictionary above.\n",
    "    return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(clt.record_dir+\"train.record\")\n",
    "train_dataset = raw_dataset.map(_parse_function)\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(clt.record_dir+\"eval.record\")\n",
    "eval_dataset = raw_dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "def map_img_label(example_proto):\n",
    "    img = tf.io.decode_jpeg(example_proto[\"image/encoded\"], channels=3)\n",
    "    img = tf.image.resize(img, (224,224))\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    label = example_proto[\"image/object/class/label\"]\n",
    "    label = tf.one_hot(label, depth=2)\n",
    "    return (img,label)\n",
    "    \n",
    "train_set = train_dataset.map(map_img_label)\n",
    "eval_set = eval_dataset.map(map_img_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SHUFFLE_BUFFER_SIZE = 50\n",
    "\n",
    "train_set = train_set.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_set = eval_set.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "model = Model(inputs = baseModel.input, outputs = headModel)\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "History = model.fit(train_set,\n",
    "    validation_data=eval_set,\n",
    "    callbacks=[tensorboard_callback],\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {k:{\"step\": History.epoch, \"value\":v} for k,v in History.history.items()}\n",
    "clt.send_logs(logs)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=opt, model=model)\n",
    "checkpoint.save(clt.checkpoint_dir+\"model.ckpt\" )\n",
    "clt.send_checkpoints()\n",
    "\n",
    "model.save(clt.exported_model)\n",
    "clt.send_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
